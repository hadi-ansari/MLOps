{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Azure ML Pipeline Tutorial using Designer and MPI components\n",
        "\n",
        "In this notebook-based tutorial, we will create and run a Azure ML pipeline\n",
        "for a simple cats vs dogs classification model.\n",
        "The pipeline will consist of the following essential components which are:\n",
        "- Convert to Image Directory (Designer component)\n",
        "- Init Image Transformation (Designer component)\n",
        "- Apply Image Transformation (Designer component)\n",
        "- Train Image Classification (MPI custom component)\n",
        "\n",
        "We followed this Azure ML [tutorial](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/pipelines/2d_image_classification_with_densenet/image_classification_with_densenet.ipynb)."
      ],
      "metadata": {
        "id": "6x1ypzczQCwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up\n",
        "We first need to install the azure-ai-ml and azureml Python packages."
      ],
      "metadata": {
        "id": "Fmgi8ZvQkScg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-ai-ml"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "as4OTe2ukSqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mldesigner"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azureml"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient, Input\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684240928218
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"SUBSCRIPTION_ID\",\n",
        "    resource_group_name=\"RESOURCE_GROUP_NAME\",\n",
        "    workspace_name=\"WORKSPACE_NAME\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684240929207
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a directory for pipeline components"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "components_dir = \"./components\"\n",
        "os.makedirs(components_dir, exist_ok=True)\n",
        "\n",
        "trainer_dir = \"./components/trainer\"\n",
        "os.makedirs(trainer_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684239977173
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up necessary variables\n",
        "\n",
        "There are some variables used to define a pipeline. You can customize these\n",
        "variables as you want. By default all output from the pipeline will be\n",
        "generated under the current directory."
      ],
      "metadata": {
        "id": "lOjDv93eS5xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "_components_dir = 'components'\n",
        "_trainer_file = os.path.join(trainer_dir, 'train_component.py')\n",
        "\n",
        "IMG_SIZE = 150 # This refers to the height and width of the images\n",
        "\n",
        "compute_instance = \"COMPUTE_INSTANCE_NAME\""
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684240933286
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_root ='../datasets/dogs_vs_cats_small'\n",
        "train_dir = os.path.join(data_root, 'train')\n",
        "validation_dir = os.path.join(data_root, 'test')\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures\n",
        "\n",
        "\n",
        "num_cats_tr = len(os.listdir(train_cats_dir))\n",
        "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
        "\n",
        "num_cats_val = len(os.listdir(validation_cats_dir))\n",
        "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
        "\n",
        "total_train = num_cats_tr + num_dogs_tr\n",
        "total_val = num_cats_val + num_dogs_val\n",
        "\n",
        "\n",
        "print('total training cat images:', num_cats_tr)\n",
        "print('total training dog images:', num_dogs_tr)\n",
        "print('total validation cat images:', num_cats_val)\n",
        "print('total validation dog images:', num_dogs_val)\n",
        "print(\"--\")\n",
        "print(\"Total training images:\", total_train)\n",
        "print(\"Total validation images:\", total_val)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "total training cat images: 102\ntotal training dog images: 100\ntotal validation cat images: 52\ntotal validation dog images: 50\n--\nTotal training images: 202\nTotal validation images: 102\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 447,
          "status": "ok",
          "timestamp": 1682412359044,
          "user": {
            "displayName": "Hadi Ansari",
            "userId": "13801521134374285736"
          },
          "user_tz": -120
        },
        "id": "aES7Hv5QTDK3",
        "outputId": "3a3cee2c-ba29-4be4-809b-8a56dc258596",
        "gather": {
          "logged": 1684240941249
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the component when it is YAML"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Load component function from yaml\n",
        "\n",
        "convert_to_image = load_component(source=\"components/convert_to_image/convert_to_image_component.yaml\")\n",
        "\n",
        "apply_transform = load_component(\n",
        "    source=\"components/apply_image_transformation/apply_image_transformation.yaml\"\n",
        ")\n",
        "\n",
        "init_transform = load_component(\n",
        "    source=\"components/init_image_transformation/init_image_transformation.yaml\"\n",
        ")\n",
        "\n",
        "# The train component is an mpi component.\n",
        "imagecnn_train = load_component(source=\"components/imagecnn_train/entry.spec.yaml\")"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684241023368
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the datasets"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = Input(name=\"TrainData\", type=\"uri_file\", path=\"../datasets/dogs_vs_cats_small/train\")\n",
        "test_ds = Input(name=\"TestData\", type=\"uri_file\", path=\"../datasets/dogs_vs_cats_small/test\")"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684242386213
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline definition"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component\n",
        "# define a pipeline containing 3 nodes: Prepare data node, train node, and score node\n",
        "@pipeline(\n",
        "    default_compute=compute_instance,\n",
        ")\n",
        "def cat_vs_dog_classifier():\n",
        "    convert_train = convert_to_image(input_path = train_ds)\n",
        "    convert_test = convert_to_image(input_path = test_ds)\n",
        "\n",
        "    init_image_transformation = init_transform(\n",
        "        resize=\"True\",\n",
        "        size=IMG_SIZE,\n",
        "        center_crop=\"False\",\n",
        "        pad=\"False\",\n",
        "        padding=0,\n",
        "        color_jitter=\"False\",\n",
        "        grayscale=\"False\",\n",
        "        random_resized_crop=\"False\",\n",
        "        random_crop=\"False\",\n",
        "        random_horizontal_flip=\"True\",\n",
        "        random_vertical_flip=\"False\",\n",
        "        random_rotation=\"False\",\n",
        "        random_affine=\"False\",\n",
        "        random_grayscale=\"False\",\n",
        "        random_perspective=\"False\",\n",
        "    )\n",
        "\n",
        "    apply_trans_on_train = apply_transform(\n",
        "    mode=\"For training\",\n",
        "    input_image_transform_path=init_image_transformation.outputs.output_path,\n",
        "    input_image_dir_path=convert_train.outputs.output_path,\n",
        "    )\n",
        "\n",
        "    apply_trans_on_val = apply_transform(\n",
        "        mode=\"For inference\",\n",
        "        input_image_transform_path=init_image_transformation.outputs.output_path,\n",
        "        input_image_dir_path=convert_test.outputs.output_path,\n",
        "    )\n",
        "\n",
        "    imagecnn_train_gpu = imagecnn_train(\n",
        "        train_data=apply_trans_on_train.outputs.output_path,\n",
        "        valid_data=apply_trans_on_val.outputs.output_path,\n",
        "        data_backend=\"pytorch\",\n",
        "        arch=\"resnet50\",\n",
        "        model_config=\"classic\",\n",
        "        workers=5,\n",
        "        epochs=4,\n",
        "        batch_size=16,\n",
        "        optimizer_batch_size=-1,\n",
        "        lr=0.1,\n",
        "        lr_schedule=\"step\",\n",
        "        warmup=0,\n",
        "        label_smoothing=0.0,\n",
        "        mixup=0.0,\n",
        "        momentum=0.9,\n",
        "        weight_decay=0.0001,\n",
        "        print_freq=10,\n",
        "        resume=\"\",\n",
        "        pretrained_weights=\"\",\n",
        "        static_loss_scale=1.0,\n",
        "        prof=-1,\n",
        "        seed=123,\n",
        "        raport_file=\"experiment_raport.json\",\n",
        "        save_checkpoint_epochs=2,\n",
        "    )\n",
        "\n",
        "    # It does not work with the GPU of type \"Standard_NC24ads_A100_v4 (24 cores, 220 GB RAM, 64 GB disk)\"\n",
        "    # imagecnn_train_gpu.compute = gpu_compute_target\n",
        "    # imagecnn_train_gpu.resources.instance_count = 1\n",
        "\n",
        "# create a pipeline\n",
        "pipeline_job = cat_vs_dog_classifier()"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684242388465
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the pipeline job"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job, experiment_name=\"dogs_vs_cats_designer_components\"\n",
        ")\n",
        "pipeline_job"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684242400180
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wait until the job completes\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684242598182
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [
        {
          "file_id": "1gGstFRenv6QgcnT_JpGBrNO_849C419y",
          "timestamp": 1682324832663
        },
        {
          "file_id": "https://github.com/tensorflow/tfx/blob/master/docs/tutorials/tfx/penguin_simple.ipynb",
          "timestamp": 1680514822420
        }
      ]
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}